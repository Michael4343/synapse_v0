Next.js and Supabase: The Chosen Stack

For this prototype, we will leverage a modern, powerful, and highly integrated technology stack: Next.js for the frontend and Supabase for the backend. Next.js, a React framework, is the ideal choice for building a scalable, high-performance application due to its robust features like Server-Side Rendering (SSR), Server Components, and a mature ecosystem. Supabase will serve as our comprehensive backend-as-a-service (BaaS), providing a PostgreSQL database, authentication, and serverless Edge Functions in a single, unified platform. This combination allows for rapid development while establishing a production-ready foundation.28

Foundational Architecture: Supabase Project and Next.js Setup

The initial phase involves establishing a robust backend infrastructure using Supabase and a corresponding frontend project with Next.js. This foundation will support user management, data storage, and the core application logic.

Project Initialization and Environment Configuration

The process begins with creating a new Supabase project via the Supabase Dashboard, which sets up a PostgreSQL database, authentication services, and auto-generated APIs.1 Upon creation, you will receive a Project URL and two essential API keys: the public
anon key for client-side use and the secret service_role key for secure server-side operations.1
Next, we initialize the Next.js project. Using the create-next-app command is the standard approach.30 For this project, we will start with a clean installation to add Supabase manually, ensuring a lean and understandable codebase.30

Bash


npx create-next-app@latest


Once the project is created, we install the necessary Supabase client libraries. The @supabase/ssr package is crucial as it is specifically designed for server-side rendering environments like Next.js, handling cookie-based authentication across the application.32

Bash


npm install @supabase/supabase-js @supabase/ssr


To manage credentials securely, we will create a .env.local file in the root of the Next.js project. This file will store the Supabase URL and the public anon key, preventing them from being committed to version control.31
An example .env.local file would be structured as follows 31:



NEXT_PUBLIC_SUPABASE_URL=YOUR_SUPABASE_URL
NEXT_PUBLIC_SUPABASE_ANON_KEY=YOUR_SUPABASE_PUBLISHABLE_KEY


These variables are then used to initialize the Supabase client within the Next.js application, ensuring a secure connection to our backend services.31

Database Schema for a Personalized Feed

With the project initialized, the next step is to define the database schema. This schema is designed for normalization and scalability, ensuring that data is organized logically and can grow with the application. The tables can be created using the Supabase Table Editor for a graphical interface or directly via the SQL Editor for more complex operations.3 The core schema consists of three tables:
profiles, submitted_urls, and feed_items.
Table 1: profiles
This table serves as an extension of Supabase's built-in auth.users table. It establishes a one-to-one relationship with the users table by using the user's unique identifier (uuid) as both its primary key and a foreign key referencing auth.users.id. This design allows for the storage of public-facing or application-specific user data separately from the secure authentication information. The table will include columns to store the narrative profile generated by the Perplexity API and a timestamp to track when the user's feed was last updated.
Table 2: submitted_urls
This table stores the list of URLs that each user provides during the onboarding process. It is a simple relational table that links each URL to a specific user via a user_id foreign key, referencing the profiles table. This allows for a one-to-many relationship, where one user can submit multiple URLs.
Table 3: feed_items
This is the central table for the application's content. It stores every individual item that appears in a user's personalized feed. Each row represents a single piece of content, such as a publication, patent, or news article. It includes a user_id to associate the item with a specific user, an item_type to categorize the content, and standard fields like title, summary, and url.
A critical architectural choice for this table is the use of a jsonb data type for a metadata column. The structured data returned by the Perplexity API will vary depending on the content type; for instance, a patent will have a patent number and inventors, while a publication will have a list of authors and a journal name. The jsonb type is a binary representation of JSON data that is highly performant and allows for flexible, schemaless storage within a structured relational database. This approach avoids the need for numerous, sparsely populated columns or separate tables for each content type, making the schema both efficient and adaptable to future changes in the data structure.3
The following table provides a detailed blueprint of the database schema.
Table Name
Column Name
Data Type
Constraints
Notes
profiles
id
uuid
Primary Key, Foreign Key (auth.users.id)
One-to-one link to the authentication user.


created_at
timestamp with time zone
Not Null, Default now()
Timestamp for profile creation.


profile_text
text
Nullable
Stores the narrative profile generated by Perplexity's Pass 1 research.


last_feed_generated_at
timestamp with time zone
Nullable
Tracks the last time the feed generation process (Pass 2) was run.
submitted_urls
id
bigint
Primary Key, Identity
Unique identifier for each submitted URL.


user_id
uuid
Not Null, Foreign Key (profiles.id)
Links the URL to a specific user profile.


url
text
Not Null
The URL submitted by the user during onboarding.


created_at
timestamp with time zone
Not Null, Default now()
Timestamp for URL submission.
feed_items
id
bigint
Primary Key, Identity
Unique identifier for each feed item.


user_id
uuid
Not Null, Foreign Key (profiles.id)
Links the feed item to a specific user profile.


item_type
text
Not Null
Category of the item (e.g., 'publication', 'patent', 'grant').


title
text
Not Null
The title of the feed item.


summary
text
Nullable
A brief summary or abstract of the item.


url
text
Nullable
A direct link to the source of the item.


metadata
jsonb
Nullable
Stores flexible, type-specific data (e.g., authors, patent number).


created_at
timestamp with time zone
Not Null, Default now()
Timestamp for when the feed item was created.

The selection of Supabase as the backend platform is a strategic decision that extends beyond its immediate utility for authentication and data storage. Its foundation on PostgreSQL provides a direct and scalable path for future enhancements. The platform's native support for advanced extensions, such as pgvector for vector embeddings and similarity search 3, is particularly noteworthy. While not implemented in the initial prototype, the
profile_text generated in the first research pass is an ideal candidate for conversion into a vector embedding. By designing the schema with this future capability in mind, a powerful user-similarity or content-recommendation feature can be added later with minimal architectural refactoring. This foresight elevates the prototype from a simple demonstration to the foundational layer of a more sophisticated, production-ready system.

Implementing User Authentication and Row Level Security (RLS)

With the schema in place, the final foundational step is to implement user authentication and secure the data. Supabase provides a comprehensive authentication service that supports various methods, including password-based sign-up and social logins.4 The
@supabase/ssr library is specifically designed to manage authentication within Next.js, handling the user's session across both client and server components via cookies.34
This requires creating two types of Supabase clients: one for Client Components (which run in the browser) and one for Server Components, Server Actions, and Route Handlers (which run on the server).34 Additionally, a Next.js middleware file is essential for refreshing expired auth tokens and ensuring the session remains valid across the application.34
The most critical security feature to implement is Row Level Security (RLS). RLS is a PostgreSQL feature that allows database administrators to define policies that restrict which rows of a table users are allowed to access.4 Without RLS, any user with the public
anon key could potentially access data belonging to other users.
For this application, RLS policies must be enabled on all three custom tables: profiles, submitted_urls, and feed_items. The policies will use the auth.uid() function, which extracts the user's ID from their JWT, to ensure that users can only interact with their own data.
Below are the SQL statements to create the necessary RLS policies:
Policy for the profiles table:

SQL


-- 1. Enable RLS on the table
ALTER TABLE public.profiles ENABLE ROW LEVEL SECURITY;

-- 2. Create policy for SELECT (read) access
CREATE POLICY "Users can view their own profile."
ON public.profiles FOR SELECT
USING (auth.uid() = id);

-- 3. Create policy for INSERT (create) access
CREATE POLICY "Users can create their own profile."
ON public.profiles FOR INSERT
WITH CHECK (auth.uid() = id);


Policy for the submitted_urls table:

SQL


-- 1. Enable RLS on the table
ALTER TABLE public.submitted_urls ENABLE ROW LEVEL SECURITY;

-- 2. Create policy for all operations
CREATE POLICY "Users can manage their own submitted URLs."
ON public.submitted_urls FOR ALL
USING (auth.uid() = user_id);


Policy for the feed_items table:

SQL


-- 1. Enable RLS on the table
ALTER TABLE public.feed_items ENABLE ROW LEVEL SECURITY;

-- 2. Create policy for SELECT (read) access
CREATE POLICY "Users can view their own feed items."
ON public.feed_items FOR SELECT
USING (auth.uid() = user_id);


(Note: INSERT operations for feed_items will be handled by a secure server-side function, so a client-side INSERT policy is not required.)
By implementing these policies from the outset, the application is secure by design. This approach prevents common data-leaking vulnerabilities that often plague prototypes built without a security-first mindset, ensuring the architecture is robust and ready for scaling.

The Intelligence Core: Mastering the Perplexity Deep Research API

The central intelligence of this application is powered by the Perplexity API, specifically its sonar-deep-research model. This is not a conventional search API that returns a list of links; it is a sophisticated research agent designed to conduct in-depth analysis and synthesize information into comprehensive reports. Understanding its unique capabilities is fundamental to architecting the application's core logic.

Understanding the sonar-deep-research Model

The sonar-deep-research model is engineered for expert-level, complex queries that require more than a simple keyword search. Its primary function is to break down a complex question into multiple, iterative sub-queries, search across hundreds of online sources, and then reason through the collected information to generate a detailed, coherent narrative.7 This process of autonomous research and synthesis is its key differentiator from standard language models or search engines.9
From a technical standpoint, the model is equipped with a large 128K context length, which allows it to process and maintain context over extensive amounts of text, making it suitable for analyzing multiple documents or URLs in a single request.10 The official documentation highlights its efficacy for tasks such as academic research, competitive intelligence, and market analysis—use cases that align perfectly with the goal of creating a high-value professional feed.10
The cost structure for the sonar-deep-research model is multi-faceted, reflecting the complexity of its operations. Billing is based on a combination of factors, including:
Input Tokens: The amount of text sent to the model in the prompt.
Output Tokens: The amount of text generated by the model in its response.
Citation Tokens: Tokens related to the sources cited in the response.
Search Queries: A per-request fee for the underlying search operations performed by the model.
Reasoning Tokens: Tokens consumed during the model's internal synthesis and reasoning process.10
A clear understanding of this pricing model is essential for managing the operational costs of the prototype, especially during development and testing phases.

API Authentication and Client Setup

Interfacing with the Perplexity API requires authentication via an API key. This key is generated within the user's account settings on the Perplexity website.11 The process involves navigating to the API section, creating an API group, and then generating a key.12 This key must be kept confidential and managed securely, for instance, as a server-side environment variable.
All API requests are made to the https://api.perplexity.ai/chat/completions endpoint. Authentication is handled by including the API key in the request header using the Bearer token scheme: Authorization: Bearer <YOUR_API_KEY>.14
The Perplexity API is designed to be compatible with the OpenAI Chat Completions format, which is a significant advantage for developers. This compatibility means that existing OpenAI client libraries (e.g., openai-python) can be used to interact with the Perplexity API by simply configuring the client to point to Perplexity's endpoint instead of OpenAI's.14
Here is an example of how to configure the OpenAI Python client for use with Perplexity:

Python


import os
from openai import OpenAI

# The API key is stored as an environment variable for security
perplexity_api_key = os.environ.get("PERPLEXITY_API_KEY")

client = OpenAI(
    api_key=perplexity_api_key,
    base_url="https://api.perplexity.ai"
)

response = client.chat.completions.create(
    model="sonar-deep-research",
    messages=,
)

print(response.choices.message.content)


This compatibility simplifies the integration process, allowing developers to leverage familiar tools and patterns when building applications on top of Perplexity's models.

Architecting the Two-Pass Research Strategy

A naive approach to building this application might involve constructing a single, highly complex prompt that instructs the model to perform all tasks at once: analyze a list of URLs, identify the user's expertise, and then find relevant publications, patents, and grants, all while formatting the output as structured JSON. However, large language models perform significantly better when complex tasks are deconstructed into smaller, more focused sub-tasks.
Consequently, this prototype employs a two-pass research architecture. This strategy deliberately separates the process into two distinct stages, each with a clear and manageable objective. This approach is not merely a technical choice but is designed to align with how the sonar-deep-research model operates and how an effective human research process unfolds.
Pass 1: Synthesis and Profile Generation
The objective of the first pass is to transform the unstructured and potentially disparate input (a list of user-provided URLs) into a single, coherent, and semantically rich artifact: the narrative user profile. This step acts as a "context-priming" phase. The model is tasked with reading all the source material and synthesizing it into a comprehensive understanding of the user's professional identity. The output is a block of text, not structured data.
Pass 2: Structured Extraction and Feed Curation
The objective of the second pass is to perform a targeted information retrieval task. It takes the highly focused context generated in Pass 1—the user profile—as its primary input. The model is then given a new set of instructions: to find specific categories of information (publications, patents, etc.) that are relevant to the person described in the profile. Crucially, this pass is constrained by a strict JSON schema to ensure the output is structured, predictable, and machine-readable.
This separation of concerns is fundamental to the reliability of the system. The sonar-deep-research model's ability to break down a query into multiple sub-searches and reason through the findings is its core strength.7 The two-pass architecture leverages this by presenting the model with two distinct and well-defined research assignments. This mirrors an effective human workflow: first, a researcher gathers background information to form a deep understanding of a topic (Pass 1); only then do they begin to search for specific data points related to that topic (Pass 2). By aligning the application's architecture with this proven research pattern, the system sets the underlying AI model up for success, yielding higher-quality and more relevant results than a single-shot approach could reliably achieve.

The Profiling Engine: Generating User Insights from URLs (Pass 1)

This section details the implementation of the first stage of the AI workflow: transforming a user's submitted URLs into a comprehensive professional profile. This process leverages sophisticated prompt engineering and a secure server-side execution environment.

Prompt Engineering for Profile Synthesis

The quality of the output from a large language model is directly proportional to the quality of the prompt. For the sonar-deep-research model, which excels with detailed instructions, a well-crafted prompt is paramount.7 The prompt for Pass 1 is designed to elicit a high-quality, narrative synthesis of the user's professional identity.
To achieve this, the prompt employs the "persona" pattern, instructing the model to adopt the role of an expert analyst. This technique helps to frame the task and guide the model's tone and focus. The prompt will contain several key instructions:
Assume a Persona: "You are a world-class professional analyst and biographer tasked with creating a detailed profile of a leading expert in their field."
Define the Input: "The following is a list of URLs associated with this expert. These may include their professional networking profile, personal website, company page, and list of publications."
State the Core Task: "Your primary objective is to thoroughly analyze the content of all provided URLs. You must synthesize the information to identify the individual's primary field of expertise, their key skills and technical competencies, their most notable projects or publications, and their current industry focus."
Specify the Output Format: "Based on your comprehensive analysis, generate a well-written narrative summary of 500-700 words. This summary should be suitable for use as a professional biography, capturing the essence of their career, contributions, and areas of influence. Do not return a list of facts; return a cohesive and polished prose summary."
This detailed prompt structure provides the model with a clear role, a defined set of inputs, an unambiguous goal, and specific constraints on the output. This level of detail is crucial for leveraging the full reasoning and synthesis capabilities of the sonar-deep-research model.

Executing the First Pass and Storing the Profile

To execute this API call securely and efficiently, a Supabase Edge Function is the ideal architectural choice. Edge Functions are server-side TypeScript functions that run on Deno, deployed globally at the edge. Using an Edge Function provides two critical benefits:
Security: The Perplexity API key is stored as an environment variable within the Supabase project and is only accessible from the server-side function. It is never exposed to the client's browser.
Orchestration: The function acts as a server-side orchestrator, managing the logic of constructing the API request, making the call to the external service (Perplexity), and then interacting with the database to store the result.
The workflow for the Pass 1 Edge Function is as follows:
The client-side Next.js application invokes the Edge Function after the user completes the onboarding process, passing the authenticated user's ID and the list of submitted URLs.
The Edge Function receives this data and constructs the request payload for the Perplexity API. It combines the engineered prompt with the list of URLs and formats it into the required messages array structure.
The function makes an authenticated POST request to the https://api.perplexity.ai/chat/completions endpoint, specifying the sonar-deep-research model.
It awaits the response from the Perplexity API and extracts the generated text content from the response body.
Finally, it uses the Supabase admin client (which has elevated privileges) to perform an UPDATE operation on the profiles table, inserting the generated profile_text into the row corresponding to the user's ID.
The narrative profile generated in this pass is more than just a transient input for the next stage; it is a valuable and persistent data asset. This synthesized text represents a condensed, semantic "fingerprint" of the user's professional identity. While its immediate purpose is to fuel the feed generation process, its potential applications are far broader. This profile could be displayed on the user's page within the application, serve as a foundation for generating customized professional documents (like a resume summary or cover letter), or, as noted previously, be converted into a vector embedding. This embedding would enable powerful features like identifying other users with similar professional backgrounds, fostering networking and collaboration. By storing this profile as a distinct entity in the database, the system creates a foundational asset that unlocks multiple future product avenues, making the initial feed a valuable but not exclusive outcome of this core process.

The Curation Engine: Generating Structured Feed Content (Pass 2)

This section represents the technical core of the prototype, detailing the process of transforming the narrative user profile into a structured, machine-readable, and personalized feed. This is achieved through advanced API features and meticulous schema design.

Advanced Prompting with Structured Outputs (JSON Mode)

To ensure the output of the second pass is reliable and can be programmatically ingested by the application, it is essential to move beyond free-form text responses. The Perplexity API provides a powerful feature for this purpose: the response_format parameter.16 This parameter allows developers to enforce a specific output structure, compelling the model to return a valid JSON object that conforms to a predefined schema.
To enable this mode, the API request must include the response_format field set to { "type": "json_object", "schema": {... } }. The schema value is a standard JSON Schema object that defines the expected structure of the output, including properties, data types, and nested objects.
Several best practices should be observed when using this feature:
Prompt Hinting: The prompt itself should include a clear instruction reinforcing the desired format, such as: "Return your response as a single, valid JSON object that strictly adheres to the provided JSON Schema. Do not include any explanatory text, markdown formatting, or other content outside of the JSON object.".17
First-Request Latency: The initial API call with a new or complex JSON schema may experience a slight delay as the model's backend prepares to handle the specific structure. Subsequent requests using the same schema will not incur this latency.16
Link Reliability: A critical consideration is the validity of URLs generated by the model within the JSON content. The model may occasionally "hallucinate" or generate broken links when forced to include them directly in a structured output. The official documentation advises that for maximum reliability, developers should rely on the links provided in the separate citations or search_results fields of the API response, rather than solely on links embedded in the JSON content.16 For this prototype, we will proceed with links in the JSON for simplicity, but this is a key area for refinement in a production system.

Designing the Feed Content JSON Schema

The JSON Schema acts as a formal contract between the application and the language model.18 It eliminates ambiguity and ensures the data returned is predictable and consistent. For this application, the schema will define a top-level object containing keys for each category of feed content. Each key will correspond to an array of objects, with each object having a defined set of properties.
The following is the JSON Schema that will be used to structure the output of Pass 2:

JSON


{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Personalized Professional Feed",
  "description": "A collection of relevant professional content tailored to a user's profile.",
  "type": "object",
  "properties": {
    "publications": {
      "description": "Recent and relevant academic or scientific publications.",
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "title": { "type": "string" },
          "authors": { "type": "array", "items": { "type": "string" } },
          "summary": { "type": "string" },
          "url": { "type": "string", "format": "uri" }
        },
        "required": ["title", "authors", "summary", "url"]
      }
    },
    "patents": {
      "description": "Recently granted or published patents in the user's field.",
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "title": { "type": "string" },
          "patent_number": { "type": "string" },
          "inventors": { "type": "array", "items": { "type": "string" } },
          "summary": { "type": "string" },
          "url": { "type": "string", "format": "uri" }
        },
        "required": ["title", "patent_number", "inventors", "summary", "url"]
      }
    },
    "funding_opportunities": {
      "description": "Relevant grants, funding announcements, or requests for proposals.",
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "title": { "type": "string" },
          "issuing_agency": { "type": "string" },
          "summary": { "type": "string" },
          "deadline": { "type": "string", "format": "date" },
          "url": { "type": "string", "format": "uri" }
        },
        "required": ["title", "issuing_agency", "summary", "url"]
      }
    },
    "trending_science_news": {
      "description": "High-impact news and developments from the user's industry or field of science.",
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "title": { "type": "string" },
          "source": { "type": "string" },
          "summary": { "type": "string" },
          "url": { "type": "string", "format": "uri" }
        },
        "required": ["title", "source", "summary", "url"]
      }
    }
  },
  "required": ["publications", "patents", "funding_opportunities", "trending_science_news"]
}


This explicit schema definition is the cornerstone of building a reliable AI-powered feature. It transforms the interaction with the LLM from a conversational request into a programmatic one, where the output structure is a non-negotiable requirement.

Executing the Feed Generation Call

The execution of Pass 2 will also be handled by a Supabase Edge Function to maintain security and separation of concerns. This function will be responsible for fetching the user's profile, constructing the complex API request for Perplexity, and processing the structured response.
The prompt for this pass will be specifically engineered for this task. It will again use the persona pattern but with a more targeted role:
"You are a hyper-specialized research assistant for a leading expert. Your task is to find the most recent and highly relevant professional information for this expert based on their detailed profile provided below. You must find content across four specific categories: academic publications, patents, funding opportunities, and trending science news. Your response must be a single, valid JSON object that strictly adheres to the provided JSON Schema."
The Edge Function's workflow will be:
Receive the user_id from the client.
Query the profiles table to retrieve the profile_text for that user.
Construct the API request payload. This will be a complex object containing:
The model set to sonar-deep-research.
The messages array, containing the system prompt and the user message (which includes the profile_text).
The response_format object, containing the json_object type and the full JSON Schema defined above.
Make the authenticated POST request to the Perplexity API.
Await and receive the structured JSON response.
By defining a strict JSON schema, the nature of the interaction with the LLM is fundamentally altered. It shifts from "prompting" in a conversational sense to "programming" the model's output. This has profound implications for system reliability. Without a schema, the LLM's output would be a string that the application would need to defensively parse, likely using fragile regular expressions or error-prone string manipulation. This creates a tight, brittle coupling between the application's logic and the unpredictable nature of a free-form text response. Using a JSON schema inverts this dependency. The application defines the data structure it requires, and the LLM is forced to conform. The resulting output can be directly deserialized into typed objects in the application code (e.g., using Zod in TypeScript), eliminating an entire class of parsing errors and making the LLM a predictable, reliable component within a larger software system.

Assembling and Delivering the Personalized Feed

The final stage of the implementation pipeline involves taking the structured JSON data generated by the Curation Engine, persisting it in the database, and making it accessible to the end-user through the Next.js application. This completes the end-to-end flow from user input to personalized content delivery.

Parsing and Persisting Feed Items

This logic resides within the same Supabase Edge Function that executes the Pass 2 API call. Once the function receives the JSON response from Perplexity, it must perform several actions to process and store the data.
First, the function must validate the response. Although using JSON mode significantly increases reliability, it is still crucial to handle potential edge cases where the model might fail to produce a valid JSON object. The code should include a try-catch block to parse the JSON string. If parsing fails, a robust system would implement a retry mechanism or log the error for manual review.17 For the prototype, logging the failure is sufficient.
Once the JSON is successfully parsed, the function will iterate through the keys of the object (publications, patents, etc.). For each item within these arrays, it will construct a new record to be inserted into the feed_items table. The item_type will be derived from the array's key (e.g., 'publication'), and the common fields (title, summary, url) will be mapped directly. Any fields that are specific to the item type (e.g., patent_number, authors) will be placed into the metadata jsonb column.
To ensure database efficiency, all the new feed items for a user should be inserted in a single transaction. The Supabase client library supports batch insertions by passing an array of objects to the insert() method. This is significantly more performant than inserting each item one by one. Before inserting new items, the function should also delete any old feed items for that user to ensure the feed is always fresh.
The final step in the Edge Function is to update the last_feed_generated_at timestamp in the user's profiles table. This provides a mechanism for rate-limiting the feed generation process and serves as a record of when the content was last refreshed.

Querying for the User Feed in Next.js

With the data securely stored, the final piece is to display it. Next.js's App Router and Server Components are perfectly suited for this task. Data fetching can happen directly on the server, ensuring the page is rendered with the necessary data before being sent to the client.36 This improves performance and security.
The feed page will be a Server Component that fetches data using the server-side Supabase client. This is where the RLS policies become critical, as they guarantee that the server-side query can only access data belonging to the currently authenticated user.38
A sample Next.js Server Component for the feed page would look like this:

TypeScript


// app/feed/page.tsx

import { createClient } from '@/utils/supabase/server';
import { cookies } from 'next/headers';

export default async function FeedPage() {
  const cookieStore = cookies();
  const supabase = createClient(cookieStore);

  // Fetch the current user session
  const {
    data: { user },
  } = await supabase.auth.getUser();

  if (!user) {
    // Handle unauthenticated user, e.g., redirect to login
    return <p>Please log in to view your feed.</p>;
  }

  // Fetch feed items for the authenticated user
  const { data: feedItems, error } = await supabase
   .from('feed_items')
   .select('*')
   .eq('user_id', user.id)
   .order('created_at', { ascending: false });

  if (error) {
    console.error('Error fetching feed items:', error);
    return <p>Could not retrieve feed.</p>;
  }

  return (
    <div>
      <h1>Your Personalized Feed</h1>
      {feedItems.map((item) => (
        <div key={item.id}>
          <h2>{item.title}</h2>
          <p>{item.summary}</p>
          <a href={item.url} target="_blank" rel="noopener noreferrer">
            Read more
          </a>
        </div>
      ))}
    </div>
  );
}


This code performs a SELECT query on the feed_items table, filtering results where the user_id matches the ID of the logged-in user. Because this is a Server Component, the data is fetched securely on the server and rendered into the HTML, completing the application's full data lifecycle.37

Strategic Recommendations and Future Trajectory

While the primary goal is a functional prototype, a forward-looking strategy is essential for evolving it into a valuable, long-term product. This involves managing operational costs and identifying a clear roadmap for future iterations.

API Cost Management and Prototyping Best Practices

The two-pass architecture, while powerful, involves two separate calls to the computationally intensive sonar-deep-research model, which can incur significant costs if not managed properly. An analysis of the Perplexity pricing model reveals several key cost drivers: input/output tokens, search queries, and reasoning tokens.10 Several strategies can be implemented to optimize for cost and efficiency.
Cache the User Profile: The narrative profile generated in Pass 1 is relatively static. A user's professional identity does not change on a daily basis. Therefore, this pass should not be executed on every login. Instead, the generated profile_text should be cached and only regenerated periodically (e.g., once a week) or when the user manually triggers a refresh, perhaps after updating their submitted URLs.
Schedule Feed Generation: Similarly, the feed generation of Pass 2 does not need to happen in real-time every time a user visits the feed page. A more cost-effective approach is to run this process on a fixed schedule. Supabase offers Cron Jobs, which can be configured to trigger the Pass 2 Edge Function at regular intervals (e.g., once every 24 hours) for all active users. This decouples API usage from user traffic and prevents redundant calls.
Optimize Prompt Length: While detail is important, prompts should be as concise as possible without sacrificing clarity. Keeping the system prompts and instructions focused can help manage the input token costs, which are a component of the overall API fee.14

A Roadmap for Future Iterations

The initial prototype establishes a strong foundation, but its true potential lies in a hybrid architecture that combines the strengths of large language models with the ground-truth data of specialized, traditional APIs.
The sonar-deep-research model is exceptional at discovery and synthesis—it can identify topics and entities that a developer might not have known to search for. However, for deep, verifiable data on those entities, specialized APIs are superior. The most powerful evolution of this prototype is to use the LLM as a "discovery and routing" layer, which then triggers calls to specific data APIs for enrichment.
For example, when the Pass 2 process identifies a relevant academic paper, the system could do more than just store the title and summary provided by the LLM. It could use the paper's title to query the Semantic Scholar API 20 or the arXiv API 22 to retrieve the official abstract, a complete list of authors, the publication venue, and the current citation count.20 If a patent is discovered, the application could use the patent number to query the USPTO PatentSearch API to pull the full filing details, legal status, and assignee information.24 If a funding opportunity is mentioned, it could query the Grants.gov API to get the official funding opportunity number, eligibility requirements, and application deadline.26
This hybrid AI/API architecture creates a "best of both worlds" system. The LLM acts as a broad, intelligent discovery engine, while the traditional APIs provide data verification, enrichment, and depth. This approach dramatically increases the value, accuracy, and reliability of the feed content, transforming it from a summary of web content into a dashboard of actionable, verified information.
Other valuable enhancements for future iterations include:
User Feedback Loop: Incorporate a simple upvote/downvote mechanism on feed items. This feedback can be collected and used to dynamically refine the Pass 2 prompt for each user over time, further personalizing the content.
Semantic Search: As mentioned earlier, by creating vector embeddings of the feed content and the user profile using a tool like Supabase's pgvector, the application can offer a powerful natural language search capability over the user's feed.
Expanded Content Types: The JSON schema can be expanded to include other high-value content categories relevant to professionals, such as upcoming academic conferences, clinical trial announcements, or in-depth industry analysis reports.

Deployment and Production Considerations with Vercel

Given the choice of Next.js, Vercel is the natural and recommended hosting provider.40 Vercel, the company behind Next.js, offers a platform that is highly optimized for deploying Next.js applications, providing seamless integration and a superior developer experience.40
Key steps for production deployment include:
Environment Variables: Securely add all necessary environment variables (Supabase URL, anon key, and the secret Perplexity API key) to the Vercel project settings. Do not commit these to your Git repository.41
Supabase Production Configuration: In the Supabase dashboard, configure the production Site URL and any additional redirect URLs under the authentication settings. This is crucial for authentication flows like magic links or social logins to work correctly in a production environment.29
Continuous Deployment: Connect your GitHub, GitLab, or Bitbucket repository to your Vercel project. This will enable automatic deployments whenever you push changes to your main branch, streamlining the release process.40
By following these steps, the prototype can be efficiently deployed to a scalable, production-ready environment, completing the development lifecycle from local setup to a live application.42
Works cited
Build a User Management App with React | Supabase Docs, accessed September 12, 2025, https://supabase.com/docs/guides/getting-started/tutorials/with-react
Getting Started | Supabase Docs, accessed September 12, 2025, https://supabase.com/docs/guides/getting-started
Database | Supabase Docs, accessed September 12, 2025, https://supabase.com/docs/guides/database/overview
Auth | Supabase Docs, accessed September 12, 2025, https://supabase.com/docs/guides/auth
How to Set Up Authentication in Your Apps with Supabase Auth - freeCodeCamp, accessed September 12, 2025, https://www.freecodecamp.org/news/set-up-authentication-in-apps-with-supabase/
Learn Supabase (Firebase Alternative) – Full Tutorial for Beginners - YouTube, accessed September 12, 2025, https://www.youtube.com/watch?v=dU7GwCOgvNY
Using Deep Research - Perplexity — Enterprise Pro, accessed September 12, 2025, https://www.perplexity.ai/enterprise/videos/using-deep-research
Introducing Perplexity Deep Research, accessed September 12, 2025, https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research
Introducing Perplexity Deep Research. Deep Research lets you generate in-depth research reports on any topic. When you ask a Deep Research a question, Perplexity performs dozens of searches, reads hundreds of sources, and reasons through the material to autonomously deliver a comprehensive report : r/perplexity_ai - Reddit, accessed September 12, 2025, https://www.reddit.com/r/perplexity_ai/comments/1ipgbib/introducing_perplexity_deep_research_deep/
Sonar deep research - Perplexity, accessed September 12, 2025, https://docs.perplexity.ai/getting-started/models/models/sonar-deep-research
What is the API? | Perplexity Help Center, accessed September 12, 2025, https://www.perplexity.ai/help-center/en/articles/10354842-what-is-the-api
Perplexity AI - Apps Documentation, accessed September 12, 2025, https://apps.make.com/perplexity-ai
API settings | Perplexity Help Center, accessed September 12, 2025, https://www.perplexity.ai/help-center/en/articles/10352995-api-settings
Perplexity API Ultimate Guide | Zuplo Learning Center, accessed September 12, 2025, https://zuplo.com/learning-center/perplexity-api
Quickstart - Perplexity API Docs, accessed September 12, 2025, https://docs.perplexity.ai/getting-started/quickstart
Structured Outputs Guide - Perplexity, accessed September 12, 2025, https://docs.perplexity.ai/guides/structured-outputs
How To Write AI Prompts That Output Valid JSON Data | Build5Nines, accessed September 12, 2025, https://build5nines.com/how-to-write-ai-prompts-that-output-valid-json-data/
Structured Prompting with JSON: The Engineering Path to Reliable LLMs | by vishal dutt | Sep, 2025 | Medium, accessed September 12, 2025, https://medium.com/@vdutt1203/structured-prompting-with-json-the-engineering-path-to-reliable-llms-2c0cb1b767cf
A Guide to JSON output with LLM prompts - YouTube, accessed September 12, 2025, https://www.youtube.com/watch?v=a9Lhm2-TgQ8
Semantic Scholar - Academic Graph API, accessed September 12, 2025, https://api.semanticscholar.org/api-docs/
Semantic Scholar Academic Graph API, accessed September 12, 2025, https://www.semanticscholar.org/product/api
Interface to the arXiv API - Docs - rOpenSci, accessed September 12, 2025, https://docs.ropensci.org/aRxiv/
lukasschwab/arxiv.py: Python wrapper for the arXiv API - GitHub, accessed September 12, 2025, https://github.com/lukasschwab/arxiv.py
API Purpose - PatentsView, accessed September 12, 2025, https://patentsview.org/apis/purpose
API - Search - Open Data Portal - USPTO, accessed September 12, 2025, https://data.uspto.gov/apis/bulk-data/search
API Guide | Grants.gov, accessed September 12, 2025, https://grants.gov/api/api-guide
API Resources | Grants.gov, accessed September 12, 2025, https://www.grants.gov/api
How to Set Up Supabase Auth in Next.js (2025 Guide) - Zestminds, accessed September 12, 2025, https://www.zestminds.com/blog/supabase-auth-nextjs-setup-guide/
Build a Powerful CRUD App with Supabase and Next.js – Full Guide - DEV Community, accessed September 12, 2025, https://dev.to/sureshramani/build-a-powerful-crud-app-with-supabase-and-nextjs-full-guide-3a6p
Initialize Next.js App with Supabase Database - Part 1 - JSTopics, accessed September 12, 2025, https://www.jstopics.com/supabase/supabase-with-next-js-part-1
Integrating Supabase with Next.js: A Step-by-Step Guide - DEV Community, accessed September 12, 2025, https://dev.to/brayancodes/integrating-supabase-with-nextjs-a-step-by-step-guide-1d85
Setting up Server-Side Auth for Next.js | Supabase Docs, accessed September 12, 2025, https://supabase.com/docs/guides/auth/server-side/nextjs
Setting Up Supabase in Next.js: A Comprehensive Guide | by Yagyaraj - Medium, accessed September 12, 2025, https://medium.com/@yagyaraj234/setting-up-supabase-in-next-js-a-comprehensive-guide-78fc6d0d738c
Build a User Management App with Next.js | Supabase Docs, accessed September 12, 2025, https://supabase.com/docs/guides/getting-started/tutorials/with-nextjs
1. Introduction to Supabase Auth with Nextjs 15 | Server Component - YouTube, accessed September 12, 2025, https://www.youtube.com/watch?v=fmXMNvxxQJQ
Fetching data from Supabase - Makerkit, accessed September 12, 2025, https://makerkit.dev/docs/next-supabase/data-fetching/fetching-data-supabase
Use Supabase with Next.js, accessed September 12, 2025, https://supabase.com/docs/guides/getting-started/quickstarts/nextjs
Next.js Server Component with Supabase (auth,data fetching,page protection) - YouTube, accessed September 12, 2025, https://www.youtube.com/watch?v=wIfWO2frpIQ
Fetching and caching Supabase data in Next.js 13 Server Components, accessed September 12, 2025, https://supabase.com/blog/fetching-and-caching-supabase-data-in-next-js-server-components
Build and deploy a Next.js app with Vercel and Supabase - Paddle Developer, accessed September 12, 2025, https://developer.paddle.com/build/nextjs-supabase-vercel-starter-kit
Deploying nextjs supabase project to Vercel - Reddit, accessed September 12, 2025, https://www.reddit.com/r/nextjs/comments/1jbmgre/deploying_nextjs_supabase_project_to_vercel/
Deploying to Production - Makerkit, accessed September 12, 2025, https://makerkit.dev/docs/next-supabase/tutorials/deploying-production
Setup a Next.js, Vercel and Supabase project in minutes | AI coding Tutorial - YouTube, accessed September 12, 2025, https://www.youtube.com/watch?v=Rv2gQX5FVg8
